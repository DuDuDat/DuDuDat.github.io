---
layout: post
title: 딥러닝 기반의 Object Detection
img: 없엉
---

> 네트워크란? 딥러닝과 컴퓨터 과학에서 다양한 방식으로 사용되는 용어로 주로 신경망을 지칭하는데 여러 개의 레이어가 서로 연결되어 복잡한 데이터 패턴을 학습할 수 있도록 구성된 시스템을 말함 일반적으로 입력 레이어, 하나 이상의 히든 레이어 그리고 출력 레이어를 포함한 전체 구조를 지칭함

### 딥러닝 기반의 Object Detection

> 주로 Convolutional Neural Network(CNN) 방식을 사용해서 객체를 감지함 여기에는 주된 접근 방식이 두가지 존재함 Two-Stage 방식과 One-Stage 방식임

### Two-Stage 방식
@@ 먼저, 후보 영역을 생성한 다음 해당 영역을 분류하는 방식 대표적으로 R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN 등이 있음

### One-Stage 방식
@@ 후보 영역을 생성하지 않고, 바로 객체를 탐지하는 방식 이 방식은 속도가 빠르지만 정확도가 약간 떨어질 수 있음 대표적으로 YOLO(You Only Look Once)와 SSD(Single Shot MultiBox Detector) 등이 있음

#### YOLO(You Only Look Once)
- 장점 : 매우 빠른 처리 속도를 제공하여 실시간 시스템에 적합함, 전체 이미지를 한번에 분석하므로 객체의 맥락을 더 잘 파악할 수 있음
- 단점 : 작은 객체나 서로 겹쳐 있는 객체들을 탐지하는데 덜 효과적일 수 있음, 초기 버전들은 정확도가 Two-Stage 방식에 비해 낮았지만 최신 버전(YOLOv4, YOLOv5)은 이러한 격차를 상당 부분 해소했다 함

#### SSD(Single Shot multiBox Detector)
- 장점 : 다양한 크기의 피쳐 맵을 사용하여 다양한 크기의 객체를 효과적으로 탐지할 수 있음, YOLO에 비해 작은 객체 탐지에서 더 나은 성능을 보임
- 단점 : YOLO보다 느릴 순 있지만 여전히 빠른 처리 속도를 제공함, 특정 상황에서는 YOLO 시리즈보다 정확도가 떨어질 수 있음

++ 특정 상황 예시 
밀집된 객체 환경 : SSD가 다양한 크기의 피쳐 맵을 활용하지만 겹치는 객체들 사이의 구분이 명확하지 않을 때 정확도가 떨어질 수 있음
극단적인 종횡비 : 매우 긴 형태의 객체나 극단적인 종횡비를 가진 개체를 탐지하는 경우 

#### RetinaNet
@@ RetinaNet은 객체 탐지를 위해 설계된 신경망 아키텍처로 특히 단일 단계(single-shot)탐지 방식을 사용합니다. 이 모델의 특징은 바로 "ResNet + FPN(Feature Pyramid Network)" 구조를 기반으로 한다는 점입니다
- 장점 : Focal Loss 라는 새로운 손실 함수를 사용하여 쉽게 분류되는 많은 수의 네거티브 샘플들에 의해 훈련이 지배되는 문제를 해결합니다. 이로 인해 어려운 객체를 더 잘 탐지할 수 있습니다, 매우 높은 정확도를 제공하며 작은 객체에 대한 탐지 능력이 우수합니다.
- 단점 : YOLO나 SSD에 비해 상대적으로 처리 속도가 느릴 수 있습니다.

++ 기본적으로 사전 훈련된 Retinanet 모델을 사용할 수 있나본디

###### Backbone Network
@@ 아래 내용을 좀 잘못된듯 이 용어의 의미는 기본적인 특징 추출 네트워크가 backbone이라고 보면 됨 특히 컨볼루션 신경망 즉, (CNN)을 사용하는 네트워크에서 자주 등장할 뿐이지 컨볼루션 신경망이 backbone인 것은 아닌 모양
@@ 주로 이미지로부터 필요한 특징들을 추출하기 위해 사용되는 것으로 이 네트워크는 일반적으로 사전에 훈련된 컨볼루션 신경망(Convolutional Neural Network, CNN)을 사용합니다. 예를 들어 VGG, FPN 또는 MobileNet과 같은 신경망이 backbone으로 사용될 수 있습니다. 이 네트워크의 목적은 입력 이미지에서 유용한 정보를 간추려내어 feature map을 생성하는 것입니다. 이 feature map은 이미지의 중요한 시각적 정보를 압축한 형태로 다음 네트워크 단계에서 사용됩니다.

###### 피처맵(특성맵) 예시 이미지
<img src="{{root_url}}/public/img/딥러닝/피처맵.png">
@@ 특성 맵이라는 것은 하나의 값이 아닌 입력 이미지에 특정 필터를 적용한 결과로서 2차원 형태의 매트릭스(행렬) 임

++ threshold : 특정 값을 설정하여 그 값 이상이나 이하일 때 특정한 동작이 일어나도록 하는 기준점을 의미한다. 간단히 말해 임계값임

##### ResNet(Residual Networks)
@@ ResNet은 깊은 신경망을 효과적으로 훈련할 수 있게 하는 핵심 아이디어인 residual learning을 도입한 아키텍처입니다. ResNet의 기본 구성 요소는 residual block이며 이 블록은 입력을 블록의 출력에 직접 더함으로써 네트워크가 더 깊어질 때 발생할 수 있는 학습 문제(예: 기울기 소실 또는 폭발)를 완화합니다. 이러한 접근방식은 네트워크가 더 깊어질수록 성능이 향상되는 것을 가능하게 합니다. 객체 탐지에서는 ResNet이 주로 특징 추출기로 사용됩니다.

++ 입력에 출력을 직접더한다 라는 개념은 잔차 학습(Residual Learning)에 대한 설명입니다. 이는 네트워크가 학습해야할 목표를 기존의 직접 출력 에서 입력과 출력의 차이로 변환한다는 것을 의미합니다. 그냥 간단하게 말해서 히든 레이어에서 학습을 시킬 때 잔차를 계속 학습시켜서 최종 출력이 원하는 결과에 더 가깝게 나오도록 한다 이 말인 거 같은데 맞나?

++ 기울기는 신경망에서 가중치에 대한 손실함수의 미분을 의미합니다. 즉, 기울기 소실은 기울기가 점점 작아져서 거의 0에 가까워지는 것을 말하고 폭발은 기울기가 너무 커져서 수치적으로 불안정해지는 현상을 말합니다.

###### BottleNeck (ResNet 구성요소 중 하나)
@@ 네트워크의 깊이를 늘리면서 파라미터의 수와 계산량을 줄이는데 도움을 주는 역할, 일반적으로 세 개의 층으로 이루어져 있는데 첫 번째와 세 번째 층에서는 1*1 크기의 컨볼루션을 사용해 채널 수를 줄이거나 늘리고 두 번째 층에서는 3*3 컨볼루션을 사용하여 실제 합성곱 연산을 처리함 이러한 구조는 입력과 출력에서 채널 수를 조절함으로써 3*3 컨볼루션 층의 부하를 감소시키고 이를 통해 더 깊은 네트워크를 효율적으로 구현할 수 있게 해줌 따라서 ResNet에서는 더 많은 레이어를 추가하면서도 전체적인 성능을 향상시킬 수 있는데 이는 복잡도를 낮추면서 학습능력을 유지할 수 있게 해줌

#### 컨볼루션 레이어(Convolutional Layer)
@@ 컨볼루션 레이어는 이미지 인식 및 처리에 매우 효과적인 신경망 레이어로 이미지의 공간적 구조를 활용하는 것이 특징입니다. 입력 이미지에서 작은 영역(커널 또는 필터라고 부름)을 슬라이딩하면서 각 위치에서 필터와 입력 데이터의 요소별 곱셈을 수행한 결과의 합을 출력으로 생성합니다. 이 과정을 통해 이미지의 특징(예: 가장자리, 질감 등)을 추출합니다.
++ 필터는 일종의 탐색 박스라고 생각할 수 있을 거 같음 필터 크기는 해당 필터가 한번에 볼 영역의 사이즈라고 보면 될 듯 이 영역의 모든 값과 필터의 가중치를 곱한 후 결과를 합해 출력 데이터의 하나의 값으로 변환한다고 함 필터의 크기가 크면 클수록 더 넓은 영역의 정보를 한번에 처리할 수 있지만 그만큼 파라미터 수와 계산량이 증가한다는 단점이 있음
++ 패딩을 하는 이유는 출력 크기 유지를 위함도 있지만 가장 중요한 것은 모서리 정보 보존을 위함인거 같음 입력 데이터의 가장자리 부분은 중앙 부분에 비해 컨볼루션 연산에서 덜 활용되기 때문에 패딩을 추가하여 가장자리 정보가 출력에서 더 잘 반영되도록 할 수 있음
++ 스트라이드는 필터가 입력 데이터를 탐색할 때 필터를 얼마나 멀리 이동시킬지 결정하는 걷임 간단히 말해 이동 거리임1 이면 필터는 한칸 2라면 두칸씩 이동함 스트라이드를 조절함으로써 출력 데이터의 크기를 조절할 수 있고 스트라이드가 크면 클수록 출력 데이터의 공간적 크기는 더 작아짐 이를 통해 다운 샘플링 효과를 낼 수 있고 필요에 따라 계산 효율성을 높일 수 있다는데 여기서 다운 샘플링이란 컴퓨터 그래픽스, 머신러닝 등에서 데이터의 해상도를 낮추거나 데이터의 양을 줄이는 과정을 말함 이 방법을 통해 계산 비용을 줄이고 메모리 사용을 최소화하며 때로는 모델의 일반화 능력을 향상시킬 수 있음

##### 컨볼루션 연산을 통한 채널 수 감소
@@ 컨볼루션 레이어(연산)을 통해 채널 수를 감소시킨다는 것은 입력 이미지 또는 피처 맵의 깊이(채널 수)를 줄이는 것을 의미합니다. 이는 커널(필터)의 개수를 조절함으로써 이루어집니다. 예를 들어 입력 채널이 2048인 개인 피쳐 맵에 256개의 1*1 컨볼루션 필터를 적용하면 출력 피처 맵의 채널 수는 256개가 됩니다. 이 연산은 주로 네트워크의 연산


##### 딥러닝에서의 다운 샘플링 방법
@@ 스트라이드 변경과 풀링 레이어 사용 등의 방법이 있음 컨볼루션 레이어에서의 스트라이드는 위에서 설명했으니 제외하고 풀링 레이어는 음 굳이 지금 할 이유는 없을 듯?

#### 활성화 함수(Activation Function)
@@ 활성화 함수는 신경망에서 입력 신호의 총합을출력 신호로 변환하는 함수로 이는 신경망의 각 노드(뉴런)에 적용되며, 신경망이 복잡한 문제를 학습할 수 있게 하는 비선형성을 제공합니다. 활성화 함수가 없다면 신경망은 선형 변환의 연속일 뿐이어서 선형 회귀와 다를 바 없게 되며 복잡한 함수를 모델링할 수 없습니다.
##### 필요한 이유
- 비선형성 제공 : 신경망이 비선형 문제 즉, 입력과 출력이 선형 관계에 있지 않은 복잡한 문제를 해결할 수 있음
- 딥러닝 모델의 표현력 증가: 더 복잡한 패턴을 학습하고 더 깊은 네트워크 구조를 만들 수 있음 이는 데이터의 높은 차원의 특성들을 효과적으로 추출하고 다양한 데이터 패턴을 인식할 수 있게 함
- ReLu와 같은 활성화 함수는 기울기(Gradient) 기반의 학습 방법, 특히 역전파 알고리즘에서 중요함, ReLu는 양수 입력에 대해 기울기가 항상 1 이므로 학습 과정 중 기울기가 사라지는 문제를 완화함

++ 역전파 알고리즘이란? 인공 신경망에서 사용되는 핵심 학습 알고리즘으로 신경망을 훈련시키는데 사용됨 이 알고리즘은 네트워크의 오류를 줄이기 위해 네트워크의 가중치를 조정하는 방법을 제공함 역전파는 신경망의 출력과 실제 라벨 간의 오차를 계산한 후 이 오차를 사용해 네트워크의 각 가중치에 대한 오차 기울기를 계산하고 이를 바탕으로 가중치를 업데이트 함

##### 역전파의 과정 (크게 두 단계로 나뉨)
- 1. 순전파(Forward Propagation): 입력 데이터는 신경망의 입력층에서부터 시작해서 각 층을 거쳐가며 각 노드의 활성화 함수를 통해 처리됨 마지막 층에서는 최종 출력값이 생성되며 이 출력값은 실제 라벨과 비교됨
- 2. 역전파(Backward Propagation): 네트워크의 출력값과 실제 라벨 간의 차이를 통해 손실을 계산함 손실 함수로는 일반적으로 평균제곱오차(MSE)나 교차 엔트로피(Cross-Entropy)등이 사용됨 손실에 대한 기울기를 계산하고 이 기울기는 네트워크를 거슬러 올라가면서 각 레이어의 가중치에 대한 기울기로 변환됨 이 과정에서 체인룰이 사용됨 각 레이어의 가중치는 계산된 기울기를 사용해 업데이트되는데 이때 학습률이라는 매개변수가 기울기의 크기를 조절하는데 사용됨

++ 체인룰(Chain Rule) : 미적분학에서 파생된 중요한 규칙으로 합성 함수의 도함수(미분값)를 구할 때 사용됨 신경망의 역전파 알고리즘에서 체인룰은 가중치에 대한 손실 함수의 기울기를 계산하는데 필수적인 역할을 함 합성함수 예시 $$h(x) = f(g(x))$$ h의 도함수를 구하기 위해 체인룰이 사용됨

##### ReLU (Rectified Linear Unit)
@@ 신경망에서 널리 사용되는 활성화 함수의 한 형태로 간단하면서도 효과적으로 신경망의 비선형성을 도입하는 역할을 함 가장 중요한 역할은 그레디언트 소실 문제를 완화하는데 도움을 줌 그레디언트 소실은 아마 기울기 소실 문제였을껄? 이건 어디 기술해놨을건데 찾아봐야됨
++ $$f(x) = max(0, x)$$ x:레이어의 입력, 양수일 땐 그 값을 그대로 출력하고 음수일 경우 0을 출력함

#### FPN(Feature Pyramid Networks)
@@ 다양한 크기의 객체를 효과적으로 탐지하기 위해 설계된 네트워크 구조입니다. 기본적으로 FPN은 하위 레벨에서 상위 레벨로 특징을 추출하는 일반적인 CNN의 계층적 구조를 활용하며 이 특징들을 피라미드 형태로 통합합니다.

++ 아마 기본적으로 세팅없이 사용하면 RetinaNet의 backbone으로써 ResNet과 FPN이 사용되는데 가벼운 모델이 필요하면 MobileNet이나 Efficientnet 과같은 모델을 사용하도록 커스텀 할 수 있다는 모양 아니면 Resnet의 더 깊은 버전을 사용해서 더 깊은 특징을 추출할 수 있다는 거 같음 FPN도 다층 특징 피라미드를 구성해서 객체의 다양한 크기를 처리하는데 중요한 역할을 하므로 FPN을 사용하는 경우가 많지만 이 역시도 다른 피라미드 구조로 대체할 수 있다는 모양 

##### 피라미드 네트워크란
@@ 순차적으로 레이어를 통과할 때마다 해상도를 줄이거나 특징을 통합하는 등의 작업을 하는데 마치 구조가 피라미드 구조처럼 변하면서 올라간다해서 피라미드 네트워크임

#### $$\alpha$$-balanced Cross Entropy
@@ 클래스 불균형을 해결하기 위한 기법으로 어떤 클래스의 예제가 다른 클래스보다 많이 나타날 수 있음을 해결하기 위한 기법입니다. 예를 들어 개와 고양이를 구분하는 모델을 훈련할 때 개의 이미지가 고양이의 이미지보다 훨씬 많을 수 있는데 이런 불균형 모델은 다수 클래스 여기서는 개에 편향되게 만들 수 있습니다. 그래서 $$\alpha$$-balanced는 각 클래스의 중요도를 조절하기 위해 사용되는 계수($$\alpha$$)를 도입합니다. $$\alpha$$ 값은 각 클래스의 샘플 수에 반비례하게 설정되어, 덜 나타나는 클래스에 더 큰 가중치를 부여하고 자주 나타나는 클래스에는 더 적은 가중치를 부여합니다. 이 방식은 클래스 간의 균형을 맞추어 모델이 모든 클래스를 공평하게 학습할 수 있도록 돕습니다.

##### Focal Loss 함수
> $$FL(p_t) = -\alpha (1-p_t) ^{\gamma} log(p_t)$$ $$p_t$$ : 모델이 정확한 클래스에 대해 예측한 확률로 예측이 올바르면 $$p_t$$ = p, 그렇지 않으면 $$p_t = 1-p$$ $$\alpha_t$$ : 알파 가중치로 클래스별로 다른 가중치를 부여하여 클래스의 불균형을 조정함 $$\gamma$$ : 감마 파람터로 이 값이 클수록 쉽게 분류된 예제들의 손실에 대한 조정을 위해 사용됨

++ 작동 방식은 모델이 틀린 예측에 대해 더 큰 손실을 부과함으로써 잘못 분류된 샘플에 더 집중하도록 하고 보다 드믄 클래스의 손실을 증가시켜 더 빈번한 클래스가 전체 학습을 지배하는 것을 방지함
@@ One-Stage 객체 탐지 방식에서 흔히 발생하는 클래스 불균형(class imbalance) 문제를 해결하기 위해 고안된 손실 함수

++ Modulating factor 는 객체 감지에서 사용되는 Focal Loss 함수의 중요한 구성요소로 기본 Cross Entropy Loss를 개선하여 쉬운 예제들이 학습에 미치는 영향을 감소시키고, 어려운 예제들에 더 많은 가중치를 주기 위해 modulating factor를 도입합니다. 이 modulating factor는 각 예제의 손실을 예제의 어려움에 따라 조절합니다.

++ 클래스 분균형(class imbalance)는 대부분의 이미지에서 배경(부정 클래스)이 객체(양성 클래스)보다 훨씬 많이 나타나는 상황을 말합니다. 이로 인해, 배경에 대한 샘플이 압도적으로 많아져 모델이 객체보다는 배경을 구분하는 데 더 많은 학습 리소스를 소모하게 되고, 결과적으로 객체 탐지 성능이 저하될 수 있습니다.

++ 피처 맵이란? 주어진 이미지에서 특징들을 추출한 것을 말함 예시보면 그 이미지 자체인데 이 이미지를 전처리해서 질감, 색상 등등을 변화시켜 여러개의 이미지로 나눈것이라 보면 될듯?

### 이미지 학습
#### 앵커박스
@@ 제대로 이해한 건지는 모르겠는데 앵커 박스 사이즈를 임의로 정해두고 시작하면 자동으로 신경망 즉, 객체 탐지 모델이 학습할 수 있는 출발점을 제공하여 이 박스들이 실제 객체의 위치와 크기, 종류 등과 일치하는지 확인하고 각 앵커박스의 사이즈를 자동으로 실제 객체와 얼마나 일치하는지 확인한후 조정이 되는 그런 개념인 모양
- 사이즈 설정과 종횡비 설정 필요 : 작은 객체부터 큰 객체까지 탐지할 수 있어야 하기 때문에 다양한 크기의 앵커 박스 설정이 필요함, 그리고 다양한 형태의 객체 역시 감지할 필요성이 있기 때문에 여러 종횡비를 가진 앵커 박스 역시 필요함

### metrics
@@ 일반적으로 신경망에서 'metrics' 라는 용어를 사용한다면 모델을 평가하기 위한 다양한 평가 지표를 의미함
#### metrics 평가 지표 종류
- Accuracy (정확도) : 전체 샘플 중 올바르게 예측된 샘플의 비율 주로 분류 문제
- Loss (손실) : 모델의 예측값과 실제 값 사이의 오찰를 계산하는 함수
- Precision (정밀도) : 양성으로 예측된 샘플 중 실제로 양성인 샘플의 비율
- 재현율 (Recall): 실제 양성 샘플 중 모델이 양성으로 정확하게 예측한 비율
- F1 점수(F1 Score) : 정밀도와 재현율의 조화 평균을 나타내며 두 지표를 동시에 고려할 때 유용함 특히 두 지표 모두 높은 값을 유지하는 것이 중요할 때 효과적으로 사용됨

++ 조화 평균이란? 여러 수의 역수들의 산술 평균의 역수로 계산됨 즉, n개의 수 $$x_1, x_2, ..., x_n$$ 에 대해 조화평균은 다음과 같음
> $$ 조화 평균 = n/ (1/x_1 + 1/x_2 + ... + 1/x_n)$$ 음 공식은 나중에 다시 찾아보자 

### __call__ 함수 개념 이건 파이썬 정리파트에 정리가 필요함
@@ 클래스에 __call__ 메서드를 구현하면 어떤 함수를 불러올지 정의할 수 있음 그래서 self.c1 = nn.Squantial(x) 인데 self.c1(y) 이런식으로 호출하는 것도 가능하게 함 이거는 그 뭐냐 아마 스트림덱 쪽에 내가 기술한 내용도 있을거임

### 프로젝트 관련 내용
- 해당 프로젝트는 ResNet 아키텍처(구조)에 backbone으로 FPN을 사용했음

### 추가 용어 정리
- scale factor : 크기를 조정하는데 사용되는 계수로 딥러닝에서는 특정 알고리즘에서 입력 데이터의 특성들을 특정 범위로 조정하는 데 사용됨
- one-hot k vector: 데이터 인코딩 방법 중 하나로 범주형 변수를 수치적으로 표현할 때 사용되며 해당 범주의 개수 k에 따라 k 차원의 벡터로 표현됨
  - 예시: 세가지 범주 A, B, C가 있는 경우 A = [1, 0, 0] B = [0, 1, 0] C = [0, 0, 1]